---
title: "[Architecture] A/B 테스트 - 데이터 기반 의사결정의 핵심"
categories:
  - Architecture
tags:
  - [ABTesting, DataDriven, ProductMetrics, Statistics, Experimentation]
---

# Introduction

---

"이 버튼을 빨간색으로 바꾸면 클릭률이 올라갈 것 같아요"라는 가설이 있을 때, 어떻게 검증할까요? 감이 아니라 **데이터로 검증**하는 방법이 바로 A/B 테스트입니다.

A/B 테스트는 두 가지 버전(A와 B)을 실제 사용자에게 나눠 보여주고, 어떤 것이 더 효과적인지 측정하는 실험 방법입니다. 제품 개선, UI/UX 최적화, 마케팅 전략 검증 등 다양한 분야에서 사용되며, Netflix, Google, Amazon 같은 기업에서는 매년 수천 건의 A/B 테스트를 실행하고 있습니다.

이 글에서는 A/B 테스트의 개념과 동작 방식, 올바른 설계 원칙, 실무에서 자주 빠지는 함정, 그리고 도구 선택까지 정리합니다. 글을 다 읽고 나면 A/B 테스트를 설계하고 결과를 해석할 때 필요한 핵심 지식을 갖추게 됩니다.

# 1. A/B 테스트란?

---

## 핵심 개념

A/B 테스트는 **통제된 실험(Controlled Experiment)**입니다. 한 가지 변수(독립변수)만 바꾸고, 나머지는 동일하게 유지한 채로 두 그룹의 결과(종속변수)를 비교합니다.

```text
전체 사용자 100명
      |
      +---> 50명: 버전 A (Control, 기존)
      |          파란색 버튼 --> 클릭률 측정
      |
      +---> 50명: 버전 B (Treatment, 변경)
                 빨간색 버튼 --> 클릭률 측정

결과: A = 5%, B = 8% --> B가 60% 더 높음 --> 통계 검증 후 B 채택
```

여기서 **Control**은 현재 운영 중인 기존 버전이고, **Treatment**는 변경을 적용한 실험 버전입니다. 두 그룹 간 차이가 우연이 아닌 변경 때문인지를 통계적으로 검증하는 것이 A/B 테스트의 핵심입니다.

## 왜 A/B 테스트인가?

```text
감에 의존하는 경우:
- "빨간색이 더 눈에 띄니까 좋을 거야" --> 실제로는 브랜드 이미지와 안 맞아서 역효과
- "텍스트를 줄이면 더 간결해" --> 실제로는 정보 부족으로 전환율 하락
- "비싼 가격을 먼저 보여주면 프리미엄 느낌" --> 실제로는 이탈률 증가

데이터로 검증하는 경우:
- 가설 수립 --> A/B 테스트 --> 데이터 분석 --> 의사결정
- 예상과 다른 결과를 발견하고 사용자 행동에 대한 인사이트 확보
- 의사결정의 근거가 명확하여 팀 간 불필요한 논쟁 감소
```

# 2. A/B 테스트 동작 방식

---

## 프로세스

A/B 테스트는 다음 6단계를 순서대로 진행합니다.

```text
1. 가설 수립
   "CTA 버튼 문구를 '가입하기'에서 '무료로 시작하기'로 바꾸면 가입률이 오를 것"
   --> 검증 가능한 형태로 구체적으로 작성

2. 메트릭 정의
   - Primary: 가입률 (Conversion Rate)
   - Secondary: 클릭률, 페이지 체류 시간
   - Guardrail: 페이지 로딩 시간, 에러율

3. 표본 크기 계산 및 트래픽 분할
   - 최소 필요 표본 크기를 미리 계산 (Power Analysis)
   - 사용자를 무작위로 A/B 그룹에 배정
   - 보통 50:50, 때로는 90:10 (위험 최소화)

4. 데이터 수집
   - 계산된 표본 크기 이상 확보 (통계적 유의성)
   - 일반적으로 최소 1~2주 실행 (요일 효과 포함)

5. 통계 분석
   - 유의수준 alpha = 0.05 기준으로 p-value 계산
   - p-value < 0.05면 "두 그룹의 차이가 우연이 아니다"라고 판단

6. 의사결정
   - 승자 선택 후 전체 트래픽에 적용
   - 또는 차이 없음 --> 원래대로 유지
```

## 핵심 통계 개념

A/B 테스트의 결과를 올바르게 해석하려면 몇 가지 통계 개념을 이해해야 합니다.

| 개념 | 의미 | 일반적 기준 |
|------|------|-------------|
| **유의수준 (alpha)** | 실제로 차이가 없는데 "있다"고 판단할 확률 (Type I Error) | 0.05 (5%) |
| **p-value** | 귀무가설이 참일 때 관측된 결과 이상의 극단값이 나올 확률 | < alpha면 유의 |
| **검정력 (Power, 1-beta)** | 실제로 차이가 있을 때 "있다"고 올바르게 판단할 확률 | 0.80 (80%) |
| **MDE** | Minimum Detectable Effect, 감지 가능한 최소 효과 크기 | 보통 상대적 5~10% |
| **신뢰구간 (CI)** | 효과 크기의 추정 범위 (95% CI가 일반적) | 0을 포함하면 비유의 |

```text
유의수준과 p-value의 관계:
  유의수준 alpha = 0.05 --> "5% 이하 확률이면 우연이 아니라고 보겠다" (미리 정하는 기준)
  p-value = 0.03       --> "우연히 이런 차이가 날 확률이 3%"
  p-value < alpha       --> 귀무가설 기각 --> "통계적으로 유의미한 차이"

Type I Error (False Positive):
  실제로 차이가 없는데 "있다"고 잘못 판단
  --> alpha로 제어 (보통 5%)

Type II Error (False Negative):
  실제로 차이가 있는데 "없다"고 잘못 판단
  --> beta로 표현, 검정력 = 1 - beta (보통 80%)
```

## 표본 크기 계산

실험을 시작하기 전에 **필요한 최소 표본 크기를 반드시 미리 계산**해야 합니다. 표본이 부족하면 실제 효과가 있어도 감지하지 못하고, 과도하면 자원을 낭비합니다.

```text
표본 크기에 영향을 주는 요소:
  1. 기존 전환율 (Baseline Rate): 50%에 가까울수록 분산이 커서 더 많은 표본 필요
  2. MDE (최소 감지 효과): 작을수록 많은 표본 필요
  3. 유의수준 (alpha): 낮을수록 많은 표본 필요
  4. 검정력 (Power): 높을수록 많은 표본 필요

실무 감각:
  기존 전환율 5%, MDE 10% (상대적, 즉 5% --> 5.5%), alpha=0.05, Power=0.80
  --> 그룹당 약 30,000명 이상 필요

  기존 전환율 5%, MDE 20% (상대적, 즉 5% --> 6%), alpha=0.05, Power=0.80
  --> 그룹당 약 8,000명 정도 필요
```

온라인 계산기(Evan Miller's Sample Size Calculator 등)를 활용하면 간편하게 계산할 수 있습니다.

## 사용자 할당 방식

사용자를 그룹에 배정할 때는 **결정론적 해싱(Deterministic Hashing)**을 사용합니다. 같은 사용자가 방문할 때마다 동일한 그룹에 배정되어야 하기 때문입니다.

```javascript
// 사용자 그룹 분할 예시 (결정론적 해싱 기반)
function assignUserToGroup(userId, experimentId) {
  // userId + experimentId를 조합하여 해싱
  // 실험마다 독립적인 그룹 배정이 가능하도록 experimentId를 포함
  const input = `${experimentId}-${userId}`;
  const hash = murmurHash3(input); // MurmurHash3 등 균일 분포 해시 함수 사용

  // 해시값을 0~1 사이의 값으로 정규화
  const normalized = (hash >>> 0) / 0xFFFFFFFF;

  if (normalized < 0.5) {
    return 'A'; // Control 그룹
  } else {
    return 'B'; // Treatment 그룹
  }
}

// 일관성 보장: 같은 userId + experimentId 조합은 항상 같은 그룹에 배정됨
// 실험 간 독립: 서로 다른 experimentId를 사용하면 그룹 배정이 독립적
```

**중요**: 사용자는 **같은 그룹에 계속 머물러야** 합니다. 방문할 때마다 다른 버전을 보면 사용자 경험이 혼란스럽고, 데이터도 오염됩니다. 이를 **일관된 사용자 경험(Consistent User Experience)**이라 합니다.

# 3. 실제 적용 예시

---

A/B 테스트가 어떤 상황에서 어떻게 사용되는지, 결과를 어떻게 해석하는지 구체적 예시로 살펴봅니다.

## 예시 1: 버튼 색상

```text
가설: 빨간색 버튼이 파란색보다 클릭률이 높을 것
변경: 버튼 색상 (파란색 --> 빨간색)
메트릭: 클릭률 (CTR)

결과:
- A (파란색): 1000명 방문, 50명 클릭 = 5.0%
- B (빨간색): 1000명 방문, 80명 클릭 = 8.0%

결론: p < 0.01, 통계적으로 유의미 --> B 채택
해석: 3%p 차이(상대적 60% 상승)로, 실무적으로도 의미 있는 개선
```

## 예시 2: 가격 표시

```text
가설: 9,900원이 10,000원보다 구매 전환율이 높을 것
변경: 가격 표시 방식
메트릭: 구매 전환율

결과:
- A (10,000원): 전환율 2.5%
- B (9,900원): 전환율 2.4%

결론: p = 0.42, 통계적으로 유의미하지 않음 --> 변경 불필요
해석: p-value가 유의수준(0.05)보다 훨씬 크므로, 관측된 차이는 우연일 가능성이 높음
```

## 예시 3: 랜딩 페이지 레이아웃

```text
가설: 이미지 중심 레이아웃이 텍스트 중심보다 이탈률이 낮을 것
변경: 랜딩 페이지 디자인
메트릭: Bounce Rate (Primary), Time on Page (Secondary)

결과:
- A (텍스트 중심): 이탈률 60%, 평균 체류 45초
- B (이미지 중심): 이탈률 50%, 평균 체류 80초

결론: p < 0.001, B가 우수 --> B 채택
해석: 이탈률 10%p 감소 + 체류 시간 78% 증가, Primary/Secondary 모두 개선
```

## 예시 4: CTA 문구

```text
가설: "무료로 시작하기"가 "가입하기"보다 가입률이 높을 것
변경: Call-to-Action 버튼 문구
메트릭: 가입률

결과:
- A ("가입하기"): 1000명 클릭, 150명 가입 = 15%
- B ("무료로 시작하기"): 1000명 클릭, 280명 가입 = 28%

결론: p < 0.01, B가 거의 2배 높음 --> B 채택
해석: "무료"라는 문구가 진입 장벽을 크게 낮춘 것으로 해석
```

# 4. A/B 테스트 설계 원칙

---

올바른 설계 없이 실행한 A/B 테스트는 잘못된 결론으로 이어집니다. 다음 5가지 원칙을 지켜야 합니다.

## 1) 한 번에 하나만 변경

```text
잘못된 예시:
  버튼 색상 + 버튼 크기 + 문구를 동시에 변경
  --> 무엇이 효과를 낳았는지 알 수 없음 (교란 변수)

올바른 예시:
  Test 1: 버튼 색상만 변경
  Test 2: 버튼 크기만 변경
  Test 3: 문구만 변경

여러 요소를 동시에 테스트하고 싶다면:
  --> Multivariate Testing(MVT)을 사용 (7장 참고)
```

## 2) 충분한 표본 크기

```text
표본이 너무 작으면:
- A: 10명 중 2명 클릭 = 20%
- B: 10명 중 3명 클릭 = 30%
--> 1명 차이로 10%p 차이가 남, 우연일 가능성 매우 높음

충분한 표본:
- A: 1000명 중 200명 클릭 = 20%
- B: 1000명 중 300명 클릭 = 30%
--> 100명 차이, 통계적으로 의미 있는 차이로 판단 가능
```

최소 표본 크기는 **통계적 검정력 분석(Power Analysis)**으로 실험 전에 계산합니다. 일반적인 기준:

| 파라미터 | 일반적 값 | 설명 |
|----------|-----------|------|
| 유의수준 (alpha) | 0.05 | 5% False Positive 허용 |
| 검정력 (Power) | 0.80 | 80% 확률로 실제 효과 감지 |
| MDE | 5~10% (상대적) | 감지하고 싶은 최소 효과 크기 |

## 3) 무작위 배정 (Randomization)

```text
편향된 배정:
  - 월요일 방문자 --> A, 화요일 방문자 --> B
  --> 요일별 사용자 특성이 다르면 결과 왜곡 (선택 편향)

  - 신규 사용자 --> A, 기존 사용자 --> B
  --> 사용자 숙련도 차이로 결과 왜곡

무작위 배정:
  - 사용자 ID 해싱으로 랜덤 그룹 분할
  - 같은 사용자는 항상 같은 그룹
  - 사용자 속성(신규/기존, 지역, 디바이스 등)이 두 그룹에 골고루 분포
```

## 4) 동시 진행

```text
순차 실행 (잘못된 방법):
  1주차: A 테스트
  2주차: B 테스트
  --> 주차별 트래픽 차이, 계절성, 외부 이벤트(명절, 세일)의 영향을 구분할 수 없음

동시 실행 (올바른 방법):
  1~2주간 A와 B를 동시에 진행
  --> 외부 변수가 양쪽에 동일하게 영향
  --> 관측된 차이가 오직 변경 사항 때문이라고 판단 가능
```

## 5) 명확한 메트릭 정의

실험을 시작하기 전에 메트릭을 세 가지 수준으로 정의합니다.

```text
Primary Metric (주요 지표):
  - 가설을 직접 검증하는 지표 (하나만 지정)
  - 예: 클릭률, 전환율, 구매액
  - 이 지표의 p-value로 실험 성패를 판단

Secondary Metrics (보조 지표):
  - 추가적인 인사이트를 얻기 위한 지표
  - 의도하지 않은 부작용 감지
  - 예: 이탈률, 페이지 체류 시간, 고객 만족도

Guardrail Metrics (가드레일):
  - 절대 나빠지면 안 되는 지표 (안전장치)
  - 이 지표가 악화되면 Primary가 개선되더라도 실험 중단
  - 예: 서버 응답 시간, 에러율, 수익
```

# 5. 실무에서 자주 하는 실수

---

A/B 테스트는 개념은 단순하지만, 실무에서는 다양한 함정이 있습니다. 아래 5가지는 가장 빈번하게 발생하는 실수입니다.

## 1) 조기 종료 (Peeking Problem)

```text
잘못된 케이스:
  Day 1: B가 A보다 20% 높네? --> 즉시 B 채택!
  Day 7: 사실 주말 트래픽 특성 때문이었고, 평균적으론 비슷함

왜 위험한가:
  - 표본이 적을 때는 결과가 크게 흔들림 (분산이 큼)
  - 매일 확인하면서 "유의미해지는 순간" 종료하면 False Positive율이 급증
  - alpha = 0.05로 설정해도, 매일 체크하면 실제 False Positive율은 20~30%까지 올라감

올바른 방법:
  - 미리 정한 기간과 표본 크기까지 실행 후 분석
  - 중간 확인이 필요하면 Sequential Testing 방법론 사용
    (예: 알파 소비 함수, 베이지안 방법 등)
```

## 2) 다중 비교 문제 (Multiple Comparison)

```text
문제:
  - 10가지 변형을 동시에 테스트
  - alpha = 0.05이면, 각 비교에서 5% 확률로 우연히 "유의"하다고 나올 수 있음
  - 비교 횟수가 많아지면 전체적으로 하나 이상이 우연히 "승자"로 보일 가능성:
    1 - (1-0.05)^10 = 약 40%

해결:
  - Bonferroni 보정: 유의수준을 비교 횟수로 나눔 (0.05/10 = 0.005)
  - Holm-Bonferroni: Bonferroni보다 덜 보수적인 단계적 보정
  - FDR (False Discovery Rate) 제어: Benjamini-Hochberg 방법
  - 또는 Multivariate Testing(MVT) 프레임워크 사용
```

## 3) Sample Ratio Mismatch (SRM)

```text
문제:
  - 50:50으로 분할했는데 실제로는 55:45 또는 60:40
  - 할당 로직의 버그, 봇 트래픽, 리다이렉트 문제 등이 원인

왜 위험한가:
  - 비율이 맞지 않으면 무작위 배정이 깨진 것
  - 결과 자체를 신뢰할 수 없음

체크:
  - 카이제곱 검정으로 실제 비율이 기대 비율과 맞는지 확인
  - 차이가 유의미하면 실험 중단하고 원인 파악
  - 실험 시작 직후부터 모니터링
```

## 4) Novelty Effect (신규 효과)

```text
문제:
  - 새로운 UI가 처음엔 호기심으로 클릭률 상승
  - 시간이 지나면 원래대로 돌아옴 (일시적 효과)

반대 현상 - Change Aversion (변화 거부):
  - 익숙한 UI가 바뀌면 일시적으로 사용이 감소
  - 적응 후에는 오히려 개선될 수 있음

해결:
  - 충분히 긴 기간 실행 (최소 1~2주, 가능하면 3~4주)
  - 시간에 따른 메트릭 변화 추이 관찰 (안정화 여부 확인)
  - 신규 사용자 vs 기존 사용자를 분리하여 분석
```

## 5) 통계적 유의성과 실무적 의미의 혼동

```text
예시:
  - A: 클릭률 10.00%
  - B: 클릭률 10.05%
  - p < 0.001 (통계적으로 유의)

하지만:
  - 0.05%p 차이는 실무적으로 의미 없을 수 있음
  - 구현 비용, 유지보수 부담, 코드 복잡도를 고려해야 함
  - 큰 표본에서는 아주 작은 차이도 "통계적으로 유의"하게 나옴

올바른 판단:
  - 통계적 유의성: "차이가 우연이 아니다"
  - 실무적 의미: "그 차이가 행동을 바꿀 만큼 크다"
  - 둘 다 확인해야 최종 의사결정이 가능
  - MDE를 미리 정해두면 실무적으로 무의미한 차이에 시간 낭비를 방지
```

# 6. A/B 테스트 도구

---

직접 구현할 수도 있지만, 이미 검증된 도구를 활용하면 통계 분석, 트래픽 분할, 결과 시각화를 쉽게 처리할 수 있습니다.

## 주요 플랫폼

| 도구 | 특징 | 대상 | 비고 |
|------|------|------|------|
| **Optimizely** | 엔터프라이즈급, Feature Experimentation | 웹/모바일 | 업계 표준 |
| **LaunchDarkly** | Feature Flag 기반 실험, 가설/메트릭/Treatment 설정 | 웹/모바일/백엔드 | DevOps 친화적 |
| **Firebase A/B Testing** | Remote Config와 연동, 모바일 최적화 | Android/iOS | 무료 |
| **VWO** | 시각적 에디터, 비개발자 친화 | 웹 | 마케터 대상 |
| **AB Tasty** | AI 기반 개인화, 시각적 에디터 | 웹 | 마케팅 중심 |
| **GrowthBook** | 오픈소스, Bayesian/Frequentist 모두 지원 | 웹/모바일/백엔드 | 자체 호스팅 가능 |

**참고**: Google Optimize는 2023년 9월에 공식 종료되었습니다. 기존 사용자는 Optimizely, VWO 등으로 마이그레이션이 필요합니다.

## Feature Flag 기반 A/B 테스트

LaunchDarkly, Optimizely 같은 최신 도구는 **Feature Flag**를 활용하여 A/B 테스트를 실행합니다. 코드에서 Feature Flag의 변형(Variation)에 따라 다른 동작을 수행하고, 플랫폼이 트래픽 분할과 결과 분석을 담당합니다.

```text
Feature Flag 기반 A/B 테스트 흐름:

  1. 플랫폼에서 실험 생성
     - Feature Flag 연결
     - 가설(Hypothesis), 메트릭(Metric), Treatment 정의
     - 각 Treatment에 트래픽 비율 할당

  2. 코드에서 Feature Flag 값에 따라 분기
     - Variation A (baseline) --> 기존 로직
     - Variation B (treatment) --> 변경된 로직

  3. 이벤트 수집 --> 플랫폼이 통계 분석 --> 결과 대시보드
```

## 자체 구현 시 고려사항

```text
필수 컴포넌트:
  1. 사용자 그룹 할당 로직 (결정론적 해싱)
  2. 변형(Variant) 전달 시스템 (API 또는 SDK)
  3. 이벤트 수집 및 저장 (이벤트 파이프라인)
  4. 통계 분석 엔진 (가설 검정, 신뢰구간 계산)
  5. 대시보드/리포팅 (실시간 모니터링)

기술 스택 예시:
  - 할당: 결정론적 해싱 (MurmurHash3 등)
  - 이벤트 수집: Google Analytics, Mixpanel, Amplitude, 자체 이벤트 파이프라인
  - 통계 분석: Python (scipy.stats, statsmodels), R
  - 시각화: Tableau, Grafana, Metabase
  - 저장: BigQuery, ClickHouse, Snowflake

주의:
  - 직접 구현하면 SRM 감지, 다중 비교 보정, Sequential Testing 등을
    모두 직접 처리해야 하므로 상당한 통계 전문성이 필요
  - 트래픽이 충분하고 실험 문화가 성숙한 조직이 아니라면 도구 활용 권장
```

# 7. 고급 기법

---

기본 A/B 테스트의 한계를 보완하는 대표적인 고급 기법 두 가지를 소개합니다.

## Multi-Armed Bandit (MAB)

```text
A/B 테스트의 한계:
  - 고정된 트래픽 분할 (50:50)
  - B가 명백히 나쁜데도 실험 종료 시까지 계속 50% 트래픽 할당
  - 실험 기간 동안 "열등한 변형"에 노출되는 사용자의 기회 비용 발생

Multi-Armed Bandit:
  - 실시간으로 성과가 좋은 쪽에 더 많은 트래픽 할당
  - Exploration (탐색) vs Exploitation (활용)의 균형

동작 방식:
  - 처음엔 50:50으로 시작
  - B가 더 좋으면 --> 40:60, 30:70으로 점진적 조정
  - 수렴하면 자연스럽게 최적 변형에 대부분의 트래픽 할당

대표 알고리즘:
  - Epsilon-Greedy: 일정 비율(epsilon)만 탐색, 나머지는 최적에 할당
  - UCB (Upper Confidence Bound): 불확실성이 큰 변형을 더 탐색
  - Thompson Sampling: 베이지안 확률 기반, 성과 분포에 따라 할당
```

## A/B 테스트 vs Multi-Armed Bandit

| 항목 | A/B 테스트 | Multi-Armed Bandit |
|------|-----------|-------------------|
| **트래픽 분할** | 고정 (50:50) | 동적 조정 |
| **통계적 엄밀성** | 높음 | 상대적으로 낮음 |
| **기회 비용** | 높음 (열등 변형에 50% 노출) | 낮음 (빠르게 최적으로 수렴) |
| **적합한 상황** | 엄밀한 인과관계 검증, 장기 실험 | 빠른 최적화, 단기 캠페인 |
| **결과 해석** | 명확한 p-value, 신뢰구간 | 누적 보상 최대화 중심 |

## Multivariate Testing (MVT)

```text
A/B 테스트: 한 번에 하나만 변경
MVT: 여러 요소를 동시에 테스트

예시:
  - 버튼 색상: 빨강 vs 파랑
  - 버튼 크기: 크게 vs 작게
  - 문구: "가입" vs "무료 시작"

조합: 2 x 2 x 2 = 8가지 변형
--> 각 요소의 독립적 효과(Main Effect) + 상호작용 효과(Interaction Effect) 측정
--> "빨간색 + 크게 + 무료 시작"이 최적이라는 결론까지 도달 가능

장점:
  - 여러 요소를 한 번의 실험으로 검증
  - 요소 간 상호작용 효과 발견 가능

단점:
  - 변형 수가 기하급수적으로 증가 (요소 3개, 각 2수준 = 8개)
  - 각 변형에 충분한 표본이 필요하므로 트래픽이 많아야 함
  - 분석이 복잡 (Factorial Design 등 통계 지식 필요)
```

# 8. 정리

---

## A/B 테스트 체크리스트

```text
실험 설계:
  - [ ] 명확하고 검증 가능한 가설이 있는가?
  - [ ] Primary/Secondary/Guardrail 메트릭을 정의했는가?
  - [ ] 한 가지만 변경하는가? (여러 변경이면 MVT 고려)
  - [ ] Power Analysis로 최소 표본 크기를 계산했는가?
  - [ ] 실험 기간을 정했는가? (보통 1~2주, 요일 효과 포함)

실험 실행:
  - [ ] 무작위 할당이 올바르게 작동하는가?
  - [ ] Sample Ratio가 예상과 일치하는가? (SRM 체크)
  - [ ] 데이터 수집이 정상 작동하는가?
  - [ ] 가드레일 메트릭이 악화되지 않는가?

결과 분석:
  - [ ] 계획된 표본 크기/기간을 충족했는가?
  - [ ] 통계적 유의성을 확인했는가? (p < 0.05)
  - [ ] 실무적으로도 의미 있는 차이인가? (MDE 이상?)
  - [ ] 세그먼트별로 다른 결과가 있는가?
  - [ ] Novelty Effect는 없는가? (시간 추이 확인)

의사결정:
  - [ ] 승자를 전체 트래픽에 적용할 준비가 됐는가?
  - [ ] 구현 비용과 효과를 비교했는가?
  - [ ] 롤백 계획이 있는가?
```

## 핵심 원칙

| 원칙 | 설명 |
|------|------|
| **감이 아닌 데이터** | 주관적 의견보다 객관적 측정 |
| **한 번에 하나** | 변수를 격리해야 인과관계 파악 가능 |
| **충분한 표본** | 통계적 검정력 확보 (Power Analysis 필수) |
| **무작위 배정** | 선택 편향 제거, 결정론적 해싱으로 일관성 유지 |
| **조급함 금지** | 미리 정한 기간과 표본 충족 후 판단 |

## 언제 A/B 테스트를 하지 말아야 하나?

```text
다음 경우엔 A/B 테스트가 적합하지 않음:

1. 트래픽이 너무 적은 경우
   --> 표본 크기를 충족할 수 없어 통계적 유의성 확보 불가
   --> 대안: 정성적 사용자 리서치 (인터뷰, 사용성 테스트)

2. 변화가 너무 급진적인 경우
   --> 전면 리뉴얼은 A/B보다 단계적 출시(Phased Rollout)가 나음
   --> 변경 요소가 너무 많아 인과관계 파악이 어려움

3. 윤리적/법적 문제가 있는 경우
   --> 사용자에게 피해를 줄 수 있는 실험은 금지
   --> 예: 건강/안전 관련 기능을 의도적으로 열화

4. 장기적 효과를 봐야 하는 경우
   --> 구독 유지율, 장기 LTV 같은 지표는 수개월이 필요
   --> A/B 테스트의 일반적인 1~2주 기간으로는 부족

5. 비용이 너무 큰 경우
   --> 구현 비용이 예상 이익보다 크면 불필요
   --> ROI를 먼저 추정하고 결정
```

```text
핵심:
  A/B 테스트 = 감이 아닌 데이터로 의사결정.
  설계가 곧 결과의 신뢰도 = 무작위 배정, 충분한 표본, 동시 진행.
  통계적 유의성과 실무적 의미를 모두 확인해야 올바른 판단.
  도구 선택은 팀 규모와 실험 문화 성숙도에 맞게.
  실수를 줄이는 가장 좋은 방법 = 사전 설계 + 체크리스트.
```

# Reference

---

- [Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing](https://exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf) - Microsoft Research (Ron Kohavi 외)
- [Sample Size Calculator](https://www.evanmiller.org/ab-testing/sample-size.html) - Evan Miller
- [Optimizely Feature Experimentation](https://docs.developers.optimizely.com/)
- [LaunchDarkly Experimentation](https://docs.launchdarkly.com/home/experimentation)
- [Firebase A/B Testing](https://firebase.google.com/docs/ab-testing)
- [GrowthBook - Open Source A/B Testing](https://www.growthbook.io/)
- [Statistics for A/B Testing](https://www.evanmiller.org/ab-testing/) - Evan Miller
- [Google Optimize Sunset (2023)](https://support.google.com/optimize/answer/12979939)
